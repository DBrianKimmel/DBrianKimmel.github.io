{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to My Documentation Site Augmented Living Development Documentation System Kubernetes (K3S) Docker Ansible Projects","title":"Home"},{"location":"#welcome-to-my-documentation-site","text":"Augmented Living Development Documentation System Kubernetes (K3S) Docker Ansible Projects","title":"Welcome to My Documentation Site"},{"location":"about/","text":"About","title":"About"},{"location":"about/#about","text":"","title":"About"},{"location":"license/","text":"License MIT License Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"license/#license","text":"","title":"License"},{"location":"license/#mit-license","text":"Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"MIT License"},{"location":"Ansible/","text":"Ansible Ansible is a neat way of automating the routine tasks of maintaining a number of computers. I started with ansible a few years ago but it got very complicated with two houses and a bunch of travel. Recently, we sold our summer home which greatly simplefies the location of the various computers. Also, I decided to set up a container system to keep things available and fault tolerant. Projects Auto Update","title":"Ansible"},{"location":"Ansible/#ansible","text":"Ansible is a neat way of automating the routine tasks of maintaining a number of computers. I started with ansible a few years ago but it got very complicated with two houses and a bunch of travel. Recently, we sold our summer home which greatly simplefies the location of the various computers. Also, I decided to set up a container system to keep things available and fault tolerant.","title":"Ansible"},{"location":"Ansible/#projects","text":"Auto Update","title":"Projects"},{"location":"Ansible/auto-update/","text":"Auto Update This will keep the computer (node) up to date with the system software (weekly). It will also keep the backups current (Monthly). ansible -b -m shell -a \"some command\"","title":"Auto Update"},{"location":"Ansible/auto-update/#auto-update","text":"This will keep the computer (node) up to date with the system software (weekly). It will also keep the backups current (Monthly). ansible -b -m shell -a \"some command\"","title":"Auto Update"},{"location":"Augmented_Living/","text":"Augmented Living In the USA and perhaps other places as well, Assisted Living is a place you go near the end of your life. So I use the term Augmented living to describe what a smart house helps you do. Instead of just automating the house, I believe the house should provide a lot more. Your house should know what it is that you want or need and provide it to you. You should not need to tell the house either by voice or changing the thermostat setting that it is too cold or too hot. If you like it cooler to sleep and you lie down in a bed somewhere in your house, that room should get or at least feel cooler. If you go into the bathroom at night while you are watching TV, the lights should illuminate your path and the bathroom. But, if you go the bathroom after being in bed, only the low illumination of night lights is needed.","title":"Augmented_Living"},{"location":"Augmented_Living/#augmented-living","text":"In the USA and perhaps other places as well, Assisted Living is a place you go near the end of your life. So I use the term Augmented living to describe what a smart house helps you do. Instead of just automating the house, I believe the house should provide a lot more. Your house should know what it is that you want or need and provide it to you. You should not need to tell the house either by voice or changing the thermostat setting that it is too cold or too hot. If you like it cooler to sleep and you lie down in a bed somewhere in your house, that room should get or at least feel cooler. If you go into the bathroom at night while you are watching TV, the lights should illuminate your path and the bathroom. But, if you go the bathroom after being in bed, only the low illumination of night lights is needed.","title":"Augmented Living"},{"location":"Development/","text":"Development Howdy.","title":"Development"},{"location":"Development/#development","text":"Howdy.","title":"Development"},{"location":"Docker/","text":"Docker I have docker running in my Kubernetes HA Cluster. Here are the projects I have running as docker continers. MkDocs","title":"Docker"},{"location":"Docker/#docker","text":"I have docker running in my Kubernetes HA Cluster. Here are the projects I have running as docker continers. MkDocs","title":"Docker"},{"location":"Docker/MkDocs/","text":"MkDocs Docs Docker Image for AMD64 docker pull squidfunk/mkdocs-material Image for ARM64 and others docker pull ghcr.io/afritzler/mkdocs-material:latest run docker run --rm -it -v ${PWD}:/docs squidfunk/mkdocs-material new . docker run --rm -it -v ${PWD}:/docs ghcr.io/afritzler/mkdocs-material:latest .","title":"MkDocs"},{"location":"Docker/MkDocs/#mkdocs","text":"Docs","title":"MkDocs"},{"location":"Docker/MkDocs/#docker","text":"Image for AMD64 docker pull squidfunk/mkdocs-material Image for ARM64 and others docker pull ghcr.io/afritzler/mkdocs-material:latest","title":"Docker"},{"location":"Docker/MkDocs/#run","text":"docker run --rm -it -v ${PWD}:/docs squidfunk/mkdocs-material new . docker run --rm -it -v ${PWD}:/docs ghcr.io/afritzler/mkdocs-material:latest .","title":"run"},{"location":"Documentation/","text":"Documentation I came across a very neat dcumentation system named MkDocs. Here is how I set it up. Development Also, I created a GitHub Pages site for all this documentation. Git Hub","title":"Documentation"},{"location":"Documentation/#documentation","text":"I came across a very neat dcumentation system named MkDocs. Here is how I set it up. Development Also, I created a GitHub Pages site for all this documentation. Git Hub","title":"Documentation"},{"location":"Documentation/development/","text":"Development I have several development computers. Keeping them updated is a pain since they run different Linux distributions. Code editing I used to use emacs but also learned vi. Later on I discovered Eclipse and switched to using it. Now I have discovered Visual Studio Code and am switching to it. VS seems to be the most natural to me and has a lot of the things I use already integrated. It also seems more natural than Eclipse. MkDocs This documentation is created using MkDocs. The software is loaded on some of the development computers. Run the following to load the documentation software. pip install mkdocs pip install mkdocs-git-revision-date-plugin pip install mkdocs-material","title":"Development"},{"location":"Documentation/development/#development","text":"I have several development computers. Keeping them updated is a pain since they run different Linux distributions.","title":"Development"},{"location":"Documentation/development/#code-editing","text":"I used to use emacs but also learned vi. Later on I discovered Eclipse and switched to using it. Now I have discovered Visual Studio Code and am switching to it. VS seems to be the most natural to me and has a lot of the things I use already integrated. It also seems more natural than Eclipse.","title":"Code editing"},{"location":"Documentation/development/#mkdocs","text":"This documentation is created using MkDocs. The software is loaded on some of the development computers. Run the following to load the documentation software. pip install mkdocs pip install mkdocs-git-revision-date-plugin pip install mkdocs-material","title":"MkDocs"},{"location":"Kubernetes/","text":"K3S Kubernetes I have elected to build a Highly Available (HA) Kubernetes cluster using Rancher K3S. The build. The 1st control node 2nd and 3rd Control nodes First Worker node Other Worker Nodes","title":"Kubernetes"},{"location":"Kubernetes/#k3s-kubernetes","text":"I have elected to build a Highly Available (HA) Kubernetes cluster using Rancher K3S. The build. The 1st control node 2nd and 3rd Control nodes First Worker node Other Worker Nodes","title":"K3S Kubernetes"},{"location":"Kubernetes/First-Control/","text":"First Control Node First we will pick out the computer we will use as the first control node. This node will have several other functions: - ansible installer for the cluster. - central log for the cluster. sudo apt install arp-scan Ansible Install Ansible sudo apt install ansible Next, we need to create a file /etc/ansible/hosts (or edit it), and add our hosts. In essence, here we define hosts and groups of hosts that Ansible will try to manage. Edit file /etc/ansible/hosts On pi-12-pp edit a hosts file for ansible. sudo nano /etc/ansible/hosts [control] control01 ansible_connection=local var_hostname=pi-12-pp control02 ansible_connection=ssh var_hostname=pi-17-pp control03 ansible_connection=ssh var_hostname=pi-18-pp [workers] cube01 ansible_connection=ssh var_hostname=pi-15-pp cube02 ansible_connection=ssh var_hostname=pi-16-pp [cube:children] control workers Master 1 In our case: pi-12-pp This is our primary node, one of 3 control nodes. We are going to install the K3s version. Use the following command to download and initialize K3s\u2019 master node. We pasted the --server into the command to tell it that we will be adding additional master nodes: If k3s has been installed kill it and remove it: sudo k3s-killall.sh sudo k3s-uninstall.sh curl -sfL https://get.k3s.io | \\ K3S_TOKEN=\"some_random_password\" \\ K3S_KUBECONFIG_MODE=\"644\" \\ sh -s - server --cluster-init --disable servicelb","title":"First Control Node"},{"location":"Kubernetes/First-Control/#first-control-node","text":"First we will pick out the computer we will use as the first control node. This node will have several other functions: - ansible installer for the cluster. - central log for the cluster. sudo apt install arp-scan","title":"First Control Node"},{"location":"Kubernetes/First-Control/#ansible","text":"","title":"Ansible"},{"location":"Kubernetes/First-Control/#install-ansible","text":"sudo apt install ansible Next, we need to create a file /etc/ansible/hosts (or edit it), and add our hosts. In essence, here we define hosts and groups of hosts that Ansible will try to manage.","title":"Install Ansible"},{"location":"Kubernetes/First-Control/#edit-file-etcansiblehosts","text":"On pi-12-pp edit a hosts file for ansible. sudo nano /etc/ansible/hosts [control] control01 ansible_connection=local var_hostname=pi-12-pp control02 ansible_connection=ssh var_hostname=pi-17-pp control03 ansible_connection=ssh var_hostname=pi-18-pp [workers] cube01 ansible_connection=ssh var_hostname=pi-15-pp cube02 ansible_connection=ssh var_hostname=pi-16-pp [cube:children] control workers","title":"Edit file /etc/ansible/hosts"},{"location":"Kubernetes/First-Control/#master-1","text":"In our case: pi-12-pp This is our primary node, one of 3 control nodes. We are going to install the K3s version. Use the following command to download and initialize K3s\u2019 master node. We pasted the --server into the command to tell it that we will be adding additional master nodes: If k3s has been installed kill it and remove it: sudo k3s-killall.sh sudo k3s-uninstall.sh curl -sfL https://get.k3s.io | \\ K3S_TOKEN=\"some_random_password\" \\ K3S_KUBECONFIG_MODE=\"644\" \\ sh -s - server --cluster-init --disable servicelb","title":"Master 1"},{"location":"Kubernetes/First-Worker/","text":"First Worker Node We need to join some workers now; in our case pi-15-pp and pi-16-pp. On every worker node do the following: curl -sfL https://get.k3s.io | \\ K3S_URL=\"https://172.16.2.12:6443\" \\ K3S_TOKEN=\"some_random_password\" sh - You can do that with ansible as well: ansible workers -b -m shell -a \"curl -sfL https://get.k3s.io | \\ K3S_URL=\"https://172.16.2.12:6443\" \\ K3S_TOKEN=\"some_random_password\" sh -\" Setting role/labels We can tag our cluster nodes to give them labels. Important k3s by default allow pods to run on the control plane, which can be OK, but in production it would not. However, in our case, we want to use disks on control nodes for storage, and that does require pods to run on them from Longhorn. So, I'll be using labels to tell pods/deployment where to run. Lets add this tag key:value: kubernetes.io/role=worker to worker nodes. This is more cosmetic, to have nice output from kubectl get nodes. kubectl label nodes pi-15-pp kubernetes.io/role=worker kubectl label nodes pi-16-pp kubernetes.io/role=worker Another label/tag. I will use this one to tell deployments to prefer nodes where node-type equals workers. The node-type is our chosen name for key, you can call it whatever. kubectl label nodes pi-15-pp node-type=worker kubectl label nodes pi-16-pp node-type=worker Whole Kubernetes cluster: kubectl get nodes NAME STATUS ROLES AGE VERSION pi-12-pp Ready etcd,master 5d3h v1.19.4+k3s1 pi-17-pp Ready etcd,master 5d3h v1.19.4+k3s1 pi-18-pp Ready etcd,master 5d3h v1.19.4+k3s1 pi-15-pp Ready worker 5d3h v1.19.4+k3s1 pi-16-pp Ready worker 5d3h v1.19.4+k3s1 You can also use kubectl get nodes --show-labels to show all labels for nodes. Lastly, add following into /etc/environment (this is so the Helm and other programs know where the Kubernetes config is.) On every node: echo \"KUBECONFIG=/etc/rancher/k3s/k3s.yaml\" >> /etc/environment Or use Ansible: ansible cube -b -m lineinfile -a \"path='/etc/environment' line='KUBECONFIG=/etc/rancher/k3s/k3s.yaml'\"","title":"First Worker Node"},{"location":"Kubernetes/First-Worker/#first-worker-node","text":"We need to join some workers now; in our case pi-15-pp and pi-16-pp. On every worker node do the following: curl -sfL https://get.k3s.io | \\ K3S_URL=\"https://172.16.2.12:6443\" \\ K3S_TOKEN=\"some_random_password\" sh - You can do that with ansible as well: ansible workers -b -m shell -a \"curl -sfL https://get.k3s.io | \\ K3S_URL=\"https://172.16.2.12:6443\" \\ K3S_TOKEN=\"some_random_password\" sh -\"","title":"First Worker Node"},{"location":"Kubernetes/First-Worker/#setting-rolelabels","text":"We can tag our cluster nodes to give them labels. Important k3s by default allow pods to run on the control plane, which can be OK, but in production it would not. However, in our case, we want to use disks on control nodes for storage, and that does require pods to run on them from Longhorn. So, I'll be using labels to tell pods/deployment where to run. Lets add this tag key:value: kubernetes.io/role=worker to worker nodes. This is more cosmetic, to have nice output from kubectl get nodes. kubectl label nodes pi-15-pp kubernetes.io/role=worker kubectl label nodes pi-16-pp kubernetes.io/role=worker Another label/tag. I will use this one to tell deployments to prefer nodes where node-type equals workers. The node-type is our chosen name for key, you can call it whatever. kubectl label nodes pi-15-pp node-type=worker kubectl label nodes pi-16-pp node-type=worker Whole Kubernetes cluster: kubectl get nodes NAME STATUS ROLES AGE VERSION pi-12-pp Ready etcd,master 5d3h v1.19.4+k3s1 pi-17-pp Ready etcd,master 5d3h v1.19.4+k3s1 pi-18-pp Ready etcd,master 5d3h v1.19.4+k3s1 pi-15-pp Ready worker 5d3h v1.19.4+k3s1 pi-16-pp Ready worker 5d3h v1.19.4+k3s1 You can also use kubectl get nodes --show-labels to show all labels for nodes. Lastly, add following into /etc/environment (this is so the Helm and other programs know where the Kubernetes config is.) On every node: echo \"KUBECONFIG=/etc/rancher/k3s/k3s.yaml\" >> /etc/environment Or use Ansible: ansible cube -b -m lineinfile -a \"path='/etc/environment' line='KUBECONFIG=/etc/rancher/k3s/k3s.yaml'\"","title":"Setting role/labels"},{"location":"Kubernetes/Other-Control/","text":"Additional Control Nodes By using the following command on both nodes, we will add these to the cluster as master nodes containing the etcd database: curl -sfL https://get.k3s.io | \\ K3S_TOKEN=\"some_random_password\" \\ K3S_KUBECONFIG_MODE=\"644\" \\ sh -s - server --server https://172.16.2.12:6443 --no-deploy servicelb You can do that with Ansible as well: ansible pi-17-pp,pi-18-pp -b -m shell -a \"curl -sfL https://get.k3s.io | \\ K3S_TOKEN=\"some_random_password\" \\ K3S_KUBECONFIG_MODE=\"644\" \\ sh -s - server --server https://172.16.2.12:6443 --no-deploy servicelb\" For the --server parameter, we are using the IP of our primary master node. This will create a control plane for our cluster. The control plane should be like this: root@control01:/home/ubuntu# kubectl get nodes NAME STATUS ROLES AGE VERSION control01 Ready etcd,master 5d3h v1.19.4+k3s1 control02 Ready etcd,master 5d3h v1.19.4+k3s1 control03 Ready etcd,master 5d3h v1.19.4+k3s1","title":"Additional Control Nodes"},{"location":"Kubernetes/Other-Control/#additional-control-nodes","text":"By using the following command on both nodes, we will add these to the cluster as master nodes containing the etcd database: curl -sfL https://get.k3s.io | \\ K3S_TOKEN=\"some_random_password\" \\ K3S_KUBECONFIG_MODE=\"644\" \\ sh -s - server --server https://172.16.2.12:6443 --no-deploy servicelb You can do that with Ansible as well: ansible pi-17-pp,pi-18-pp -b -m shell -a \"curl -sfL https://get.k3s.io | \\ K3S_TOKEN=\"some_random_password\" \\ K3S_KUBECONFIG_MODE=\"644\" \\ sh -s - server --server https://172.16.2.12:6443 --no-deploy servicelb\" For the --server parameter, we are using the IP of our primary master node. This will create a control plane for our cluster. The control plane should be like this: root@control01:/home/ubuntu# kubectl get nodes NAME STATUS ROLES AGE VERSION control01 Ready etcd,master 5d3h v1.19.4+k3s1 control02 Ready etcd,master 5d3h v1.19.4+k3s1 control03 Ready etcd,master 5d3h v1.19.4+k3s1","title":"Additional Control Nodes"},{"location":"Kubernetes/Other-Worker/","text":"Additional Worker Nodes","title":"Additional Worker Nodes"},{"location":"Kubernetes/Other-Worker/#additional-worker-nodes","text":"","title":"Additional Worker Nodes"},{"location":"Kubernetes/Features/","text":"Features Here are some various features that I have put into my Kubernetes HA Cluster. Central Logging Longhorn","title":"Features"},{"location":"Kubernetes/Features/#features","text":"Here are some various features that I have put into my Kubernetes HA Cluster. Central Logging Longhorn","title":"Features"},{"location":"Kubernetes/Features/CentralLogging/","text":"Central Logging Server Pick a server. I used pi-12-pp which is the first control node also. On Logging Server Create a place to hold all the logs. sudo mkdir /var/log/central sudo chown syslog:adm /var/log/central Rsyslog will use TCP/UDP port 514, but you need to enable it. Edit /etc/rsyslog.conf, and make sure these lines look like this (by uncommenting them): sudo nano /etc/rsyslog.conf # provides UDP syslog reception module(load=\"imudp\") input(type=\"imudp\" port=\"514\") # provides TCP syslog reception module(load=\"imtcp\") input(type=\"imtcp\" port=\"514\") Next create config to tell rsyslog to put all logs in previously created folder, create /etc/rsyslog.d/45-central.conf sudo nano /etc/rsyslog.d/45-central.conf $template RemoteLogs,\"/var/log/central/%HOSTNAME%.log\" *.* ?RemoteLogs This will put all logs under /var/log/central/ .log Last thing, and this is kind of optional, we need to tell logrotate about this, and have it rotate the logs, so you don't end up with 100+MB text files. Create file /etc/logrotate.d/central /var/log/central/*.log { rotate 4 weekly missingok notifempty compress delaycompress sharedscripts postrotate invoke-rc.d rsyslog rotate >/dev/null 2>&1 || true endscript } rotate - How many rotated copies to keep before removing the oldest one. weekly - Rotate log every 7 days. missingok - If the log file is missing, go on to the next one without issuing an error message. notifempty - Do not rotate the log if it is empty. compress - Gzip the logs. delaycompress - Postpone compression of the previous log file to the next rotation cycle. sharedscripts - Because we are going to use wildcard, we need this argument, telling logrotate this setting is for multiple logs. postrotate - What to do after rotation is finished, in this case invoke rsyslog rotate. Some more info about options: https://linux.die.net/man/8/logrotate Restart rsyslog sudo systemctl restart rsyslog That\u2019s it for a server, no need to restart logrotate; that will be run via cron. lnav Just a nifty little program to watch your logs in real time, with filters and so on. sudo apt install lnav lnav /var/log/central/*.log","title":"Central Logging"},{"location":"Kubernetes/Features/CentralLogging/#central-logging","text":"","title":"Central Logging"},{"location":"Kubernetes/Features/CentralLogging/#server","text":"Pick a server. I used pi-12-pp which is the first control node also.","title":"Server"},{"location":"Kubernetes/Features/CentralLogging/#on-logging-server","text":"Create a place to hold all the logs. sudo mkdir /var/log/central sudo chown syslog:adm /var/log/central Rsyslog will use TCP/UDP port 514, but you need to enable it. Edit /etc/rsyslog.conf, and make sure these lines look like this (by uncommenting them): sudo nano /etc/rsyslog.conf # provides UDP syslog reception module(load=\"imudp\") input(type=\"imudp\" port=\"514\") # provides TCP syslog reception module(load=\"imtcp\") input(type=\"imtcp\" port=\"514\") Next create config to tell rsyslog to put all logs in previously created folder, create /etc/rsyslog.d/45-central.conf sudo nano /etc/rsyslog.d/45-central.conf $template RemoteLogs,\"/var/log/central/%HOSTNAME%.log\" *.* ?RemoteLogs This will put all logs under /var/log/central/ .log Last thing, and this is kind of optional, we need to tell logrotate about this, and have it rotate the logs, so you don't end up with 100+MB text files. Create file /etc/logrotate.d/central /var/log/central/*.log { rotate 4 weekly missingok notifempty compress delaycompress sharedscripts postrotate invoke-rc.d rsyslog rotate >/dev/null 2>&1 || true endscript } rotate - How many rotated copies to keep before removing the oldest one. weekly - Rotate log every 7 days. missingok - If the log file is missing, go on to the next one without issuing an error message. notifempty - Do not rotate the log if it is empty. compress - Gzip the logs. delaycompress - Postpone compression of the previous log file to the next rotation cycle. sharedscripts - Because we are going to use wildcard, we need this argument, telling logrotate this setting is for multiple logs. postrotate - What to do after rotation is finished, in this case invoke rsyslog rotate. Some more info about options: https://linux.die.net/man/8/logrotate Restart rsyslog sudo systemctl restart rsyslog That\u2019s it for a server, no need to restart logrotate; that will be run via cron.","title":"On Logging Server"},{"location":"Kubernetes/Features/CentralLogging/#lnav","text":"Just a nifty little program to watch your logs in real time, with filters and so on. sudo apt install lnav lnav /var/log/central/*.log","title":"lnav"},{"location":"Kubernetes/Features/Longhorn/","text":"Longhorn There is an issue with Raspberry Pi / Ubuntu: the names for disks are assigned almost at random. Therefore, even if you have USB disks in the same slots on multiple nodes, they can be named at random, /dev/sda or /dev/sdb, and there is no easy way to enforce the naming, without messing with udev rules a lot. Identifying disks for storage We are going to use Ansible again a lot, and will add new variables with disk names that will be used for storage into /etc/ansible/hosts. We are going to use the lsblk -f command on every node and look for disk labels: ansible cube -b -m shell -a \"lsblk -f\" Results: pi-12-pp | CHANGED | rc=0 >> NAME FSTYPE LABEL UUID FSAVAIL FSUSE% MOUNTPOINT sda \u251c\u2500sda1 vfat 302E-C0DF \u251c\u2500sda2 ext4 ssd-04-arm32 9006c7c6-4791-43f1-bc65-5262c472f64b \u251c\u2500sda3 ext4 ssd-04-arm64 ad3c04f1-4cea-4118-bb71-cd937960c9a9 3.8G 71% / \u2514\u2500sda4 LVM2_member WYao6H-RtuF-J6e3-D1VE-UENC-vWZc-tNx0js \u2514\u2500vg--ctl--01-lv--store ext4 1e3438fb-1ce4-4c3b-a709-b397708fdc06 18.5G 0% /var/storage mmcblk0 \u251c\u2500mmcblk0p1 vfat system-boot 4D3B-86C0 135.9M 46% /boot/firmware \u2514\u2500mmcblk0p2 ext4 writable 79af43d1-801b-4c28-81d5-724c930bcc83 11G 19% /SD pi-17-pp | CHANGED | rc=0 >> NAME FSTYPE LABEL UUID FSAVAIL FSUSE% MOUNTPOINT sda \u251c\u2500sda1 vfat 49CF-1F5A \u251c\u2500sda2 ext4 ssd-07-arm32 461215ce-ea53-4996-8697-54420e7a1da3 \u251c\u2500sda3 ext4 ssd-07-arm64 b0ac490f-5f83-4ed1-9b94-8a52436b955f 10.3G 29% / \u2514\u2500sda4 LVM2_member fppIPT-cI0k-Ki17-3ajV-j40Z-toBg-ICwV69 \u2514\u2500vg--ctl--02-lv_store ext4 fd92218f-9c8a-4e9c-bb06-8d3704bd9b0a 18.5G 0% /var/storage mmcblk0 \u251c\u2500mmcblk0p1 vfat system-boot 4D3B-86C0 132.8M 47% /boot/firmware \u2514\u2500mmcblk0p2 ext4 writable 79af43d1-801b-4c28-81d5-724c930bcc83 10G 26% /SD pi-15-pp | CHANGED | rc=0 >> NAME FSTYPE LABEL UUID FSAVAIL FSUSE% MOUNTPOINT sda \u251c\u2500sda1 exfat 7C8B-3E4C \u251c\u2500sda2 ext4 ssd-05-arm32 f0ac0c45-7ab2-4f11-8373-7a2f568a3bc2 \u251c\u2500sda3 ext4 ssd-05-arm64 91f02013-d751-4182-8bcb-5bf6f54498b7 10.4G 28% / \u2514\u2500sda4 LVM2_member 4pM34i-8p3I-6BNB-LgX3-rc0Q-ICh1-tz5Mzs \u2514\u2500vg_wrk--01-lv_store ext4 69a82734-94e5-4c03-b5d0-99aa5cfcc794 18.5G 0% /var/storage mmcblk0 \u251c\u2500mmcblk0p1 vfat system-boot 4D3B-86C0 132.8M 47% /boot/firmware \u2514\u2500mmcblk0p2 ext4 writable 79af43d1-801b-4c28-81d5-724c930bcc83 23G 16% /SD pi-16-pp | CHANGED | rc=0 >> NAME FSTYPE LABEL UUID FSAVAIL FSUSE% MOUNTPOINT sda \u251c\u2500sda1 exfat 18D0-56F4 \u251c\u2500sda2 ext4 ssd-06-arm32 352ed1b1-4a1e-4cc5-b0e3-c9c3cb34215d \u251c\u2500sda3 ext4 ssd-06-arm64 33ad9dc7-82c8-4f49-9521-b0590044d68c 10.6G 27% / \u2514\u2500sda4 LVM2_member X7LxNz-0G1w-ZAAi-uiXQ-WwWK-ZEvR-r6R16z \u2514\u2500vg--wrk--02-lv_stor ext4 23ab704c-0fb8-4d38-824c-8168ca1d8602 18.5G 0% /var/storage mmcblk0 \u251c\u2500mmcblk0p1 vfat system-boot 4D3B-86C0 132.8M 47% /boot/firmware \u2514\u2500mmcblk0p2 ext4 writable 79af43d1-801b-4c28-81d5-724c930bcc83 10.9G 19% /SD pi-18-pp | CHANGED | rc=0 >> NAME FSTYPE LABEL UUID FSAVAIL FSUSE% MOUNTPOINT sda \u251c\u2500sda1 \u251c\u2500sda2 ext4 ssd-08-arm32 b7a16d06-0811-43d3-9326-e1b0f0a00b61 \u251c\u2500sda3 ext4 ssd-08-arm64 e8ab8e92-a75b-4905-8660-d7a88075514d 11.4G 22% / \u2514\u2500sda4 LVM2_member UJzcYL-4740-IGbL-szaG-T7xh-WA0C-I3cgA6 \u2514\u2500vg_ctl_03-lv_stor ext4 04383936-778a-4b6c-bb8c-850ebfe042a4 18.5G 0% /var/storage mmcblk0 \u251c\u2500mmcblk0p1 vfat system-boot 4D3B-86C0 132.8M 47% /boot/firmware \u2514\u2500mmcblk0p2 ext4 writable 79af43d1-801b-4c28-81d5-724c930bcc83 10.6G 22% /SD Edit /etc/ansible/hosts, and add a new variable (I have chosen name var_disk) with the disk to wipe. TAKE YOUR TIME AND LOOK TWICE ! the wipefs command we\u2019re gonna use will not wipe your OS disk but it might wipe any other that is not mounted. [control] control01 ansible_connection=local var_hostname=control01 var_disk=sda control02 ansible_connection=ssh var_hostname=control02 var_disk=sdb control03 ansible_connection=ssh var_hostname=control03 var_disk=sdb [workers] cube01 ansible_connection=ssh var_hostname=cube01 var_disk=sdb cube02 ansible_connection=ssh var_hostname=cube02 var_disk=sdb cube03 ansible_connection=ssh var_hostname=cube03 var_disk=sdb cube04 ansible_connection=ssh var_hostname=cube04 var_disk=sdb cube05 ansible_connection=ssh var_hostname=cube05 var_disk=sdb cube06 ansible_connection=ssh var_hostname=cube06 var_disk=sda [cube:children] control workers","title":"Longhorn"},{"location":"Kubernetes/Features/Longhorn/#longhorn","text":"There is an issue with Raspberry Pi / Ubuntu: the names for disks are assigned almost at random. Therefore, even if you have USB disks in the same slots on multiple nodes, they can be named at random, /dev/sda or /dev/sdb, and there is no easy way to enforce the naming, without messing with udev rules a lot.","title":"Longhorn"},{"location":"Kubernetes/Features/Longhorn/#identifying-disks-for-storage","text":"We are going to use Ansible again a lot, and will add new variables with disk names that will be used for storage into /etc/ansible/hosts. We are going to use the lsblk -f command on every node and look for disk labels: ansible cube -b -m shell -a \"lsblk -f\" Results: pi-12-pp | CHANGED | rc=0 >> NAME FSTYPE LABEL UUID FSAVAIL FSUSE% MOUNTPOINT sda \u251c\u2500sda1 vfat 302E-C0DF \u251c\u2500sda2 ext4 ssd-04-arm32 9006c7c6-4791-43f1-bc65-5262c472f64b \u251c\u2500sda3 ext4 ssd-04-arm64 ad3c04f1-4cea-4118-bb71-cd937960c9a9 3.8G 71% / \u2514\u2500sda4 LVM2_member WYao6H-RtuF-J6e3-D1VE-UENC-vWZc-tNx0js \u2514\u2500vg--ctl--01-lv--store ext4 1e3438fb-1ce4-4c3b-a709-b397708fdc06 18.5G 0% /var/storage mmcblk0 \u251c\u2500mmcblk0p1 vfat system-boot 4D3B-86C0 135.9M 46% /boot/firmware \u2514\u2500mmcblk0p2 ext4 writable 79af43d1-801b-4c28-81d5-724c930bcc83 11G 19% /SD pi-17-pp | CHANGED | rc=0 >> NAME FSTYPE LABEL UUID FSAVAIL FSUSE% MOUNTPOINT sda \u251c\u2500sda1 vfat 49CF-1F5A \u251c\u2500sda2 ext4 ssd-07-arm32 461215ce-ea53-4996-8697-54420e7a1da3 \u251c\u2500sda3 ext4 ssd-07-arm64 b0ac490f-5f83-4ed1-9b94-8a52436b955f 10.3G 29% / \u2514\u2500sda4 LVM2_member fppIPT-cI0k-Ki17-3ajV-j40Z-toBg-ICwV69 \u2514\u2500vg--ctl--02-lv_store ext4 fd92218f-9c8a-4e9c-bb06-8d3704bd9b0a 18.5G 0% /var/storage mmcblk0 \u251c\u2500mmcblk0p1 vfat system-boot 4D3B-86C0 132.8M 47% /boot/firmware \u2514\u2500mmcblk0p2 ext4 writable 79af43d1-801b-4c28-81d5-724c930bcc83 10G 26% /SD pi-15-pp | CHANGED | rc=0 >> NAME FSTYPE LABEL UUID FSAVAIL FSUSE% MOUNTPOINT sda \u251c\u2500sda1 exfat 7C8B-3E4C \u251c\u2500sda2 ext4 ssd-05-arm32 f0ac0c45-7ab2-4f11-8373-7a2f568a3bc2 \u251c\u2500sda3 ext4 ssd-05-arm64 91f02013-d751-4182-8bcb-5bf6f54498b7 10.4G 28% / \u2514\u2500sda4 LVM2_member 4pM34i-8p3I-6BNB-LgX3-rc0Q-ICh1-tz5Mzs \u2514\u2500vg_wrk--01-lv_store ext4 69a82734-94e5-4c03-b5d0-99aa5cfcc794 18.5G 0% /var/storage mmcblk0 \u251c\u2500mmcblk0p1 vfat system-boot 4D3B-86C0 132.8M 47% /boot/firmware \u2514\u2500mmcblk0p2 ext4 writable 79af43d1-801b-4c28-81d5-724c930bcc83 23G 16% /SD pi-16-pp | CHANGED | rc=0 >> NAME FSTYPE LABEL UUID FSAVAIL FSUSE% MOUNTPOINT sda \u251c\u2500sda1 exfat 18D0-56F4 \u251c\u2500sda2 ext4 ssd-06-arm32 352ed1b1-4a1e-4cc5-b0e3-c9c3cb34215d \u251c\u2500sda3 ext4 ssd-06-arm64 33ad9dc7-82c8-4f49-9521-b0590044d68c 10.6G 27% / \u2514\u2500sda4 LVM2_member X7LxNz-0G1w-ZAAi-uiXQ-WwWK-ZEvR-r6R16z \u2514\u2500vg--wrk--02-lv_stor ext4 23ab704c-0fb8-4d38-824c-8168ca1d8602 18.5G 0% /var/storage mmcblk0 \u251c\u2500mmcblk0p1 vfat system-boot 4D3B-86C0 132.8M 47% /boot/firmware \u2514\u2500mmcblk0p2 ext4 writable 79af43d1-801b-4c28-81d5-724c930bcc83 10.9G 19% /SD pi-18-pp | CHANGED | rc=0 >> NAME FSTYPE LABEL UUID FSAVAIL FSUSE% MOUNTPOINT sda \u251c\u2500sda1 \u251c\u2500sda2 ext4 ssd-08-arm32 b7a16d06-0811-43d3-9326-e1b0f0a00b61 \u251c\u2500sda3 ext4 ssd-08-arm64 e8ab8e92-a75b-4905-8660-d7a88075514d 11.4G 22% / \u2514\u2500sda4 LVM2_member UJzcYL-4740-IGbL-szaG-T7xh-WA0C-I3cgA6 \u2514\u2500vg_ctl_03-lv_stor ext4 04383936-778a-4b6c-bb8c-850ebfe042a4 18.5G 0% /var/storage mmcblk0 \u251c\u2500mmcblk0p1 vfat system-boot 4D3B-86C0 132.8M 47% /boot/firmware \u2514\u2500mmcblk0p2 ext4 writable 79af43d1-801b-4c28-81d5-724c930bcc83 10.6G 22% /SD Edit /etc/ansible/hosts, and add a new variable (I have chosen name var_disk) with the disk to wipe. TAKE YOUR TIME AND LOOK TWICE ! the wipefs command we\u2019re gonna use will not wipe your OS disk but it might wipe any other that is not mounted. [control] control01 ansible_connection=local var_hostname=control01 var_disk=sda control02 ansible_connection=ssh var_hostname=control02 var_disk=sdb control03 ansible_connection=ssh var_hostname=control03 var_disk=sdb [workers] cube01 ansible_connection=ssh var_hostname=cube01 var_disk=sdb cube02 ansible_connection=ssh var_hostname=cube02 var_disk=sdb cube03 ansible_connection=ssh var_hostname=cube03 var_disk=sdb cube04 ansible_connection=ssh var_hostname=cube04 var_disk=sdb cube05 ansible_connection=ssh var_hostname=cube05 var_disk=sdb cube06 ansible_connection=ssh var_hostname=cube06 var_disk=sda [cube:children] control workers","title":"Identifying disks for storage"},{"location":"Projects/","text":"Projects PiHole DNS MkDocs","title":"Projects"},{"location":"Projects/#projects","text":"PiHole DNS MkDocs","title":"Projects"}]}